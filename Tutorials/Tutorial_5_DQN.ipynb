{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn7PKu9r0asK"
      },
      "source": [
        "#Tutorial 5 - DQN\n",
        "\n",
        "Please follow this tutorial to understand the structure (code) of DQN algorithm.\n",
        "\n",
        "\n",
        "### References:\n",
        "\n",
        "Please follow [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) for the original publication as well as the psuedocode. Watch Prof. Ravi's lectures on moodle or nptel for further understanding of the core concepts. Contact the TAs for further resources if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azUjb7UK4Yfh",
        "outputId": "1a01512c-bca2-4722-f243-c492590a1bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.1.1)\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Installing packages for rendering the game on Colab\n",
        "'''\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "!pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_DODRgW_ZKS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNA5kiH_esJ",
        "outputId": "774bf49b-0534-466e-8202-7d836da953f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "2\n",
            "1\n",
            "----\n",
            "[ 0.01369617 -0.02302133 -0.04590265 -0.04834723]\n",
            "----\n",
            "1\n",
            "----\n",
            "[ 0.01323574  0.17272775 -0.04686959 -0.3551522 ]\n",
            "1.0\n",
            "False\n",
            "{}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Please refer to the first tutorial for more details on the specifics of environments\n",
        "We've only added important commands you might find useful for experiments.\n",
        "'''\n",
        "\n",
        "'''\n",
        "List of example environments\n",
        "(Source - https://gym.openai.com/envs/#classic_control)\n",
        "\n",
        "'Acrobot-v1'\n",
        "'Cartpole-v1'\n",
        "'MountainCar-v0'\n",
        "'''\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "no_of_actions = env.action_space.n\n",
        "\n",
        "print(state_shape)\n",
        "print(no_of_actions)\n",
        "print(env.action_space.sample())\n",
        "print(\"----\")\n",
        "\n",
        "'''\n",
        "# Understanding State, Action, Reward Dynamics\n",
        "\n",
        "The agent decides an action to take depending on the state.\n",
        "\n",
        "The Environment keeps a variable specifically for the current state.\n",
        "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
        "- It returns the new current state and reward for the agent to take the next action\n",
        "\n",
        "'''\n",
        "\n",
        "state = env.reset()\n",
        "''' This returns the initial state (when environment is reset) '''\n",
        "\n",
        "print(state)\n",
        "print(\"----\")\n",
        "\n",
        "action = env.action_space.sample()\n",
        "''' We take a random action now '''\n",
        "\n",
        "print(action)\n",
        "print(\"----\")\n",
        "\n",
        "next_state, reward, done, info = env.step(action)\n",
        "''' env.step is used to calculate new state and obtain reward based on old state and action taken  '''\n",
        "\n",
        "print(next_state)\n",
        "print(reward)\n",
        "print(done)\n",
        "print(info)\n",
        "print(\"----\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuaOxavDXus"
      },
      "source": [
        "## DQN\n",
        "\n",
        "Using NNs as substitutes isn't something new. It has been tried earlier, but the 'human control' paper really popularised using NNs by providing a few stability ideas (Q-Targets, Experience Replay & Truncation). The 'Deep-Q Network' (DQN) Algorithm can be broken down into having the following components.\n",
        "\n",
        "### Q-Network:\n",
        "The neural network used as a function approximator is defined below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4MRC1p2DZbp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "### Q Network & Some 'hyperparameters'\n",
        "\n",
        "QNetwork1:\n",
        "Input Layer - 4 nodes (State Shape) \\\n",
        "Hidden Layer 1 - 128 nodes \\\n",
        "Hidden Layer 2 - 64 nodes \\\n",
        "Output Layer - 2 nodes (Action Space) \\\n",
        "Optimizer - zero_grad()\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "'''\n",
        "Bunch of Hyper parameters (Which you might have to tune later)\n",
        "'''\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "LR = 5e-4               # learning rate\n",
        "UPDATE_EVERY = 20       # how often to update the network (When Q target is present)\n",
        "\n",
        "\n",
        "class QNetwork1(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork1, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmv5c0XoK8GA"
      },
      "source": [
        "### Replay Buffer:\n",
        "\n",
        "Recall why we use such a technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8VJYkqoLqlO"
      },
      "source": [
        "## Tutorial Agent Code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok_5eQM7OCTj"
      },
      "outputs": [],
      "source": [
        "class TutorialAgent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        ''' Agent Environment Interaction '''\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        ''' Q-Network '''\n",
        "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        ''' Replay memory '''\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        ''' Save experience in replay memory '''\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        ''' If enough samples are available in memory, get random subset and learn '''\n",
        "        if len(self.memory) >= BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
        "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "\n",
        "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        ''' Epsilon-greedy action selection (Already Present) '''\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ''' Get max predicted Q values (for next states) from target model'''\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        ''' Compute Q targets for current states '''\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        ''' Get expected Q values from local model '''\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        ''' Compute loss '''\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        ''' Minimize the loss '''\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        ''' Gradiant Clipping '''\n",
        "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "        for param in self.qnetwork_local.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQFbRCHWQyO"
      },
      "source": [
        "### Here, we present the DQN algorithm code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6A2TdUHWVUN",
        "outputId": "f71f4797-cbac-418d-a9de-c0739f9b5ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 38.24\n",
            "Episode 200\tAverage Score: 163.10\n",
            "Episode 221\tAverage Score: 195.58\n",
            "Environment solved in 221 episodes!\tAverage Score: 195.58\n",
            "0:01:19.568590\n"
          ]
        }
      ],
      "source": [
        "''' Defining DQN Algorithm '''\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "\n",
        "\n",
        "def dqn(n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "    scores_window = deque(maxlen=100)\n",
        "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "\n",
        "    eps = eps_start\n",
        "    ''' initialize epsilon '''\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "        ''' decrease epsilon '''\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=195.0:\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "    return True\n",
        "\n",
        "''' Trial run to check if algorithm runs and saves the data '''\n",
        "\n",
        "begin_time = datetime.datetime.now()\n",
        "\n",
        "agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
        "dqn()\n",
        "\n",
        "time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "print(time_taken)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL9YMq9yPHLk"
      },
      "source": [
        "### **Task 1a**  \n",
        "Understand the core of the algorithm, follow the flow of data. Identify the exploration strategy used.\n",
        "### **Task 1b**\n",
        "Out of the two exploration strategies discussed in class ($Ïµ$-greedy & Softmax). Implement the strategy that's not used here.\n",
        "### **Task 1c**\n",
        "How fast does the agent 'solve' the environment in terms of the number of episodes?\n",
        "(Cartpole-v1 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials)\n",
        "\n",
        "How 'well' does the agent learn? (reward plot?) The above two are some 'evaluation metrics' you can use to comment on the performance of an algorithm.\n",
        "\n",
        "Please compare DQN (using $\\epsilon$-greedy) with DQN (using softmax). Think along the lines of 'no. of episodes', 'reward plots', 'compute time', etc. and add a few comments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LBh6_lOVBdN"
      },
      "source": [
        "#### **Submission Steps**\n",
        "\n",
        "#### Task 1: Add a text cell with the answer.\n",
        "\n",
        "#### Task 2: Add a code cell below task 1 solution and use 'Tutorial Agent Code' to build your new agent (with a different exploration strategy).\n",
        "\n",
        "#### Task 3: Add a code cell below task 2 solution running both the agents to solve the CartPole v-1 environment and add a new text cell below it with your inferences."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}