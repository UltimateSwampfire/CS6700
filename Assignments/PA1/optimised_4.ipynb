{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "def row_col_to_seq(row_col, num_cols):  #Converts state number to row_column format\n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "def seq_to_col_row(seq, num_cols): #Converts row_column format to state number\n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False,gamma=1):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = gamma # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "\n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "\n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "\n",
    "            p += self.P[state, next_state, action]\n",
    "\n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "\n",
    "          arr = self.P[next_state, :, 3]\n",
    "          next_next = np.where(arr == np.amax(arr))\n",
    "          next_next = next_next[0][0]\n",
    "          return next_next, self.R[next_next]\n",
    "        else:\n",
    "          return next_state, self.R[next_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def sarsa(model, alpha=0.5, epsilon=0.1, maxiter=100, maxeps=1000, temperature=1.0, action_strategy='epsilon-greedy'):\n",
    "    \"\"\"\n",
    "    Solves the supplied environment using SARSA.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : python object\n",
    "        Holds information about the environment to solve\n",
    "        such as the reward structure and the transition dynamics.\n",
    "\n",
    "    alpha : float\n",
    "        Algorithm learning rate. Defaults to 0.5.\n",
    "\n",
    "    epsilon : float\n",
    "        Probability that a random action is selected. epsilon must be\n",
    "        in the interval [0,1] where 0 means that the action is selected\n",
    "        in a completely greedy manner and 1 means the action is always\n",
    "        selected randomly.\n",
    "\n",
    "    maxiter : int\n",
    "        The maximum number of iterations to perform per episode.\n",
    "        Defaults to 100.\n",
    "\n",
    "    maxeps : int\n",
    "        The number of episodes to run SARSA for.\n",
    "        Defaults to 1000.\n",
    "\n",
    "    temperature : float, optional\n",
    "        Controls the level of exploration. Higher temperature leads to more\n",
    "        exploration, while lower temperature makes the action selection more\n",
    "        deterministic. Defaults to 1.0.\n",
    "\n",
    "    action_strategy : str, optional\n",
    "        Action selection strategy: 'epsilon-greedy' or 'softmax'.\n",
    "        Defaults to 'epsilon-greedy'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q : numpy array of shape (N, 1)\n",
    "        The state-action value for the environment where N is the\n",
    "        total number of states.\n",
    "\n",
    "    pi : numpy array of shape (N, 1)\n",
    "        Optimal policy for the environment where N is the total\n",
    "        number of states.\n",
    "\n",
    "    state_counts : numpy array of shape (N, 1)\n",
    "        Counts of the number of times each state is visited.\n",
    "\n",
    "    total_steps_per_episode : list\n",
    "        List containing the total steps per episode.\n",
    "\n",
    "    total_rewards_per_episode : list\n",
    "        List containing the total rewards per episode.\n",
    "    \"\"\"\n",
    "    # initialize the state-action value function and the state counts\n",
    "    Q = np.zeros((model.num_states, model.num_actions))\n",
    "    state_counts = np.zeros((model.num_states, 1))\n",
    "    \n",
    "    total_steps_per_episode = []\n",
    "    total_rewards_per_episode = []\n",
    "\n",
    "    for i in range(maxeps):\n",
    "\n",
    "        if np.mod(i, 1000) == 0:\n",
    "            print(\"Running episode %i.\" % i)\n",
    "\n",
    "        # for each new episode, start at the given start state\n",
    "        state = int(model.start_state_seq)\n",
    "        # sample first action\n",
    "        action = sample_action(Q, state, model.num_actions, epsilon, temperature, action_strategy)\n",
    "        episode_steps = 0\n",
    "        episode_rewards = 0\n",
    "\n",
    "        for j in range(maxiter):\n",
    "            # initialize p and r\n",
    "            p, r = 0, np.random.random()\n",
    "            # sample the next state according to the action and the\n",
    "            # probability of the transition\n",
    "            for next_state in range(model.num_states):\n",
    "                p += model.P[state, next_state, action]\n",
    "                if r <= p:\n",
    "                    break\n",
    "            # sample the next action\n",
    "            next_action = sample_action(Q, next_state, model.num_actions, epsilon, temperature, action_strategy)\n",
    "            # Calculate the temporal difference and update Q function\n",
    "            Q[state, action] += alpha * (model.R[state] + model.gamma * Q[next_state, next_action] - Q[state, action])\n",
    "            # End episode is state is a terminal state\n",
    "            episode_steps += 1\n",
    "            episode_rewards += model.R[state]\n",
    "\n",
    "            if np.any(state == model.goal_states_seq):\n",
    "                break\n",
    "\n",
    "            # count the state visits\n",
    "            state_counts[state] += 1\n",
    "\n",
    "            # store the previous state and action\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        total_steps_per_episode.append(int(episode_steps))\n",
    "        total_rewards_per_episode.append(int(episode_rewards))\n",
    "\n",
    "    # determine the q function and policy\n",
    "    q = np.max(Q, axis=1).reshape(-1, 1)\n",
    "    pi = np.argmax(Q, axis=1).reshape(-1, 1)\n",
    "\n",
    "    return q, pi, state_counts, total_steps_per_episode, total_rewards_per_episode\n",
    "\n",
    "def sample_action(Q, state, num_actions, epsilon, temperature=1.0, action_strategy='epsilon-greedy'):\n",
    "    \"\"\"\n",
    "    Action selection based on the specified strategy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : numpy array of shape (N, num_actions)\n",
    "        Q function for the environment where N is the total number of states.\n",
    "\n",
    "    state : int\n",
    "        The current state.\n",
    "\n",
    "    num_actions : int\n",
    "        The number of actions.\n",
    "\n",
    "    epsilon : float\n",
    "        Probability that a random action is selected. epsilon must be\n",
    "        in the interval [0,1] where 0 means that the action is selected\n",
    "        in a completely greedy manner and 1 means the action is always\n",
    "        selected randomly.\n",
    "\n",
    "    temperature : float, optional\n",
    "        Controls the level of exploration. Higher temperature leads to more\n",
    "        exploration, while lower temperature makes the action selection more\n",
    "        deterministic. Defaults to 1.0.\n",
    "\n",
    "    action_strategy : str, optional\n",
    "        Action selection strategy: 'epsilon-greedy' or 'softmax'.\n",
    "        Defaults to 'epsilon-greedy'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    action : int\n",
    "        Number representing the selected action between 0 and num_actions.\n",
    "    \"\"\"\n",
    "    if action_strategy == 'epsilon-greedy':\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(0, num_actions)\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "    elif action_strategy == 'softmax':\n",
    "        action_values = Q[state, :]\n",
    "        probabilities = softmax(action_values / temperature)\n",
    "        action = np.random.choice(np.arange(num_actions), p=probabilities)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action_strategy. Use 'epsilon-greedy' or 'softmax'.\")\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def qlearning(model, alpha=0.5, epsilon=0.1, maxiter=100, maxeps=1000, temperature=1.0, action_strategy='epsilon-greedy'):\n",
    "    \"\"\"\n",
    "    Solves the supplied environment using Q-learning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : python object\n",
    "        Holds information about the environment to solve\n",
    "        such as the reward structure and the transition dynamics.\n",
    "\n",
    "    alpha : float\n",
    "        Algorithm learning rate. Defaults to 0.5.\n",
    "\n",
    "    epsilon : float\n",
    "        Probability that a random action is selected. epsilon must be\n",
    "        in the interval [0,1] where 0 means that the action is selected\n",
    "        in a completely greedy manner and 1 means the action is always\n",
    "        selected randomly.\n",
    "\n",
    "    maxiter : int\n",
    "        The maximum number of iterations to perform per episode.\n",
    "        Defaults to 100.\n",
    "\n",
    "    maxeps : int\n",
    "        The number of episodes to run Q-learning for.\n",
    "        Defaults to 1000.\n",
    "\n",
    "    temperature : float, optional\n",
    "        Controls the level of exploration. Higher temperature leads to more\n",
    "        exploration, while lower temperature makes the action selection more\n",
    "        deterministic. Defaults to 1.0.\n",
    "\n",
    "    action_strategy : str, optional\n",
    "        Action selection strategy: 'epsilon-greedy' or 'softmax'.\n",
    "        Defaults to 'epsilon-greedy'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q : numpy array of shape (N, 1)\n",
    "        The state-action value for the environment where N is the\n",
    "        total number of states.\n",
    "\n",
    "    pi : numpy array of shape (N, 1)\n",
    "        Optimal policy for the environment where N is the total\n",
    "        number of states.\n",
    "\n",
    "    state_counts : numpy array of shape (N, 1)\n",
    "        Counts of the number of times each state is visited.\n",
    "\n",
    "    total_steps_per_episode : list\n",
    "        List containing the total steps per episode.\n",
    "\n",
    "    total_rewards_per_episode : list\n",
    "        List containing the total rewards per episode.\n",
    "    \"\"\"\n",
    "    Q = np.zeros((model.num_states, model.num_actions))\n",
    "    state_counts = np.zeros((model.num_states, 1))\n",
    "\n",
    "    total_steps_per_episode = []\n",
    "    total_rewards_per_episode = []\n",
    "\n",
    "    for i in range(maxeps):\n",
    "        if np.mod(i, 1000) == 0:\n",
    "            print(\"Running episode %i.\" % i)\n",
    "\n",
    "        state = int(model.start_state_seq)\n",
    "        episode_steps = 0\n",
    "        episode_rewards = 0\n",
    "\n",
    "        for j in range(maxiter):\n",
    "            action = sample_action(Q, state, model.num_actions, epsilon, temperature, action_strategy)\n",
    "\n",
    "            p, r = 0, np.random.random()\n",
    "\n",
    "            for next_state in range(model.num_states):\n",
    "                p += model.P[state, next_state, action]\n",
    "                if r <= p:\n",
    "                    break\n",
    "\n",
    "            Q[state, action] += alpha * (model.R[state] + model.gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "\n",
    "            state_counts[state] += 1\n",
    "\n",
    "            state = next_state\n",
    "            episode_steps += 1\n",
    "            episode_rewards += model.R[state]\n",
    "\n",
    "            if np.any(state == model.goal_states_seq):\n",
    "                break\n",
    "\n",
    "        total_steps_per_episode.append(int(episode_steps))\n",
    "        total_rewards_per_episode.append(int(episode_rewards))\n",
    "\n",
    "    q = np.max(Q, axis=1).reshape(-1, 1)\n",
    "    pi = np.argmax(Q, axis=1).reshape(-1, 1)\n",
    "\n",
    "    return q, pi, state_counts, total_steps_per_episode, total_rewards_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "restart_states = np.array([[3,7],[8,2]])\n",
    "start_state = np.array([[3,6]])\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "# create model\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                    bad_states=bad_states,\n",
    "                    restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "               goal_reward=10,\n",
    "               bad_state_reward=-6,\n",
    "               restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=0.7,\n",
    "                              bias=0.5)\n",
    "\n",
    "env_0 = gw.create_gridworld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 0.5, log=True)\n",
    "    epsilon = trial.suggest_float('epsilon', 0.1, 0.5, log=True)\n",
    "    g=trial.suggest_float('gamma',0.8,1,log=True)\n",
    "\n",
    "    print(f\"Trial {trial.number}: Alpha = {alpha}, Epsilon = {epsilon},Gamma={g}\")\n",
    "\n",
    "    num_expts = 5\n",
    "    total_rewards, total_steps, Q_func, state_visit = [], [], [], []\n",
    "    gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False,gamma=g)\n",
    "    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                            bad_states=bad_states,\n",
    "                            restart_states=restart_states)\n",
    "    gw.add_rewards(step_reward=-1,\n",
    "                    goal_reward=10,\n",
    "                    bad_state_reward=-6,\n",
    "                    restart_state_reward=-100)\n",
    "    gw.add_transition_probability(p_good_transition=0.7,\n",
    "                                    bias=0.5)\n",
    "\n",
    "    env_0 = gw.create_gridworld()\n",
    "\n",
    "    for i in range(num_expts):\n",
    "        print(\"Experiment: %d\" % (i + 1))\n",
    "\n",
    "        q_function, pi, state_counts, steps, rewards = sarsa(env_0, alpha=alpha, epsilon=epsilon, maxiter=100, maxeps=5000)\n",
    "        total_steps.append(steps)\n",
    "        total_rewards.append(rewards)\n",
    "        Q_func.append(q_function)\n",
    "        state_visit.append(state_counts)\n",
    "\n",
    "    # Define a weighted combination of minimizing steps and maximizing rewards\n",
    "    # Adjust the weights based on your preference\n",
    "    objective_value = 0.5 * np.sum(np.sum(np.array(total_steps)),axis=0) / num_expts + 0.5 * (-np.sum(np.sum(np.array(total_rewards))) / num_expts)\n",
    "\n",
    "    return objective_value\n",
    "\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the combined objective\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_alpha = study.best_params['alpha']\n",
    "best_epsilon = study.best_params['epsilon']\n",
    "best_gamma=study.best_params['gamma']\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(\"Best Alpha:\", best_alpha)\n",
    "print(\"Best Epsilon:\", best_epsilon)\n",
    "print(\"Best gamma:\", best_gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_expts = 5\n",
    "total_rewards, total_steps,Q_func,state_visit = [], [],[],[]\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False,gamma=best_gamma)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                        bad_states=bad_states,\n",
    "                        restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "                goal_reward=10,\n",
    "                bad_state_reward=-6,\n",
    "                restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=0.7,\n",
    "                                bias=0.5)\n",
    "\n",
    "env_0 = gw.create_gridworld()\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "\n",
    "    # TODO: run sarsa, store metrics\n",
    "    #Q, episode_rewards, steps_to_completion = qlearning(env, Q, choose_action=choose_action_softmax)\n",
    "    q_function, pi, state_counts,steps,rewards = sarsa(env_0, alpha=best_alpha, epsilon=best_epsilon, maxiter=100, maxeps=5000)\n",
    "    total_steps.append(steps)\n",
    "    total_rewards.append(rewards)\n",
    "    Q_func.append(q_function)\n",
    "    state_visit.append(state_counts)\n",
    "\n",
    "\n",
    "#steps_average=sum(steps)/len(steps)\n",
    "#reward_average=sum(rewards)/len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_steps=np.mean(np.array(total_steps), axis=0)\n",
    "std_across_steps= np.std(np.array(total_steps), axis=0)\n",
    "\n",
    "avg_rewards=np.mean(np.array(total_rewards), axis=0)\n",
    "std_across_rewards= np.std(np.array(total_rewards), axis=0)\n",
    "\n",
    "avg_q=np.mean(np.array(Q_func), axis=0)\n",
    "avg_state=np.mean(np.array(state_visit), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming avg_steps, std_across_steps, avg_rewards, and std_across_rewards are defined\n",
    "\n",
    "episodes = np.arange(1, len(avg_steps) + 1)\n",
    "\n",
    "# Only include every 25th episode\n",
    "sampled_episodes = episodes[::10]\n",
    "sampled_avg_steps = avg_steps[::10]\n",
    "sampled_std_across_steps = std_across_steps[::10]\n",
    "sampled_avg_rewards = avg_rewards[::10]\n",
    "sampled_std_across_rewards = std_across_rewards[::10]\n",
    "\n",
    "# sampled_episodes = episodes\n",
    "# sampled_avg_steps = avg_steps\n",
    "# sampled_std_across_steps = std_across_steps\n",
    "# sampled_avg_rewards = avg_rewards\n",
    "# sampled_std_across_rewards = std_across_rewards\n",
    "\n",
    "# Plot for mean steps\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sampled_episodes, sampled_avg_steps, label='Mean Steps', color='blue')\n",
    "plt.fill_between(sampled_episodes, sampled_avg_steps - sampled_std_across_steps, \n",
    "                 sampled_avg_steps + sampled_std_across_steps, alpha=0.2, color='blue')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Steps')\n",
    "plt.title('Mean Steps and Standard Deviation Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot for mean rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sampled_episodes, sampled_avg_rewards, label='Mean Rewards', color='green')\n",
    "plt.fill_between(sampled_episodes, sampled_avg_rewards - sampled_std_across_rewards, \n",
    "                 sampled_avg_rewards + sampled_std_across_rewards, alpha=0.2, color='green')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Rewards')\n",
    "plt.title('Mean Rewards and Standard Deviation Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gridworld(env_0, policy=pi, state_counts=avg_state, title=\"SARSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_gridworld(env_0, policy=pi,value_function=avg_q, title=\"SARSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 0.5, log=True)\n",
    "    temp = trial.suggest_float('temperature',0.1,1, log=True)\n",
    "    g=trial.suggest_float('gamma',0.8,1,log=True)\n",
    "\n",
    "    print(f\"Trial {trial.number}: Alpha = {alpha}, Temperature = {temp},Gamma={g}\")\n",
    "\n",
    "    num_expts = 5\n",
    "    total_rewards, total_steps, Q_func, state_visit = [], [], [], []\n",
    "    gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False,gamma=g)\n",
    "    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                            bad_states=bad_states,\n",
    "                            restart_states=restart_states)\n",
    "    gw.add_rewards(step_reward=-1,\n",
    "                    goal_reward=10,\n",
    "                    bad_state_reward=-6,\n",
    "                    restart_state_reward=-100)\n",
    "    gw.add_transition_probability(p_good_transition=0.7,\n",
    "                                    bias=0.5)\n",
    "\n",
    "    env_0 = gw.create_gridworld()\n",
    "\n",
    "    for i in range(num_expts):\n",
    "        print(\"Experiment: %d\" % (i + 1))\n",
    "\n",
    "        q_function, pi, state_counts,steps,rewards = sarsa(env_0, alpha=alpha, epsilon=0.2, maxiter=100, maxeps=5000,action_strategy=\"softmax\",temperature=temp)\n",
    "\n",
    "        total_steps.append(steps)\n",
    "        total_rewards.append(rewards)\n",
    "        Q_func.append(q_function)\n",
    "        state_visit.append(state_counts)\n",
    "\n",
    "    # Define a weighted combination of minimizing steps and maximizing rewards\n",
    "    # Adjust the weights based on your preference\n",
    "    objective_value = 0.5 * np.sum(np.sum(np.array(total_steps)),axis=0) / num_expts + 0.5 * (-np.sum(np.sum(np.array(total_rewards))) / num_expts)\n",
    "\n",
    "    return objective_value\n",
    "\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the combined objective\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_alpha = study.best_params['alpha']\n",
    "best_temp = study.best_params['temperature']\n",
    "best_gamma=study.best_params['gamma']\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(\"Best Alpha:\", best_alpha)\n",
    "print(\"Best temp:\", best_temp)\n",
    "print(\"Best gamma:\", best_gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_expts = 5\n",
    "total_rewards, total_steps,Q_func,state_visit = [], [],[],[]\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False,gamma=best_gamma)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                        bad_states=bad_states,\n",
    "                        restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "                goal_reward=10,\n",
    "                bad_state_reward=-6,\n",
    "                restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=0.7,\n",
    "                                bias=0.5)\n",
    "\n",
    "env_0 = gw.create_gridworld()\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "\n",
    "    # TODO: run sarsa, store metrics\n",
    "    #Q, episode_rewards, steps_to_completion = qlearning(env, Q, choose_action=choose_action_softmax)\n",
    "    q_function, pi, state_counts,steps,rewards = sarsa(env_0, alpha=best_alpha, epsilon=0.2, maxiter=100, maxeps=5000,action_strategy=\"softmax\",temperature=best_temp)\n",
    "    total_steps.append(steps)\n",
    "    total_rewards.append(rewards)\n",
    "    Q_func.append(q_function)\n",
    "    state_visit.append(state_counts)\n",
    "\n",
    "\n",
    "#steps_average=sum(steps)/len(steps)\n",
    "#reward_average=sum(rewards)/len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_steps=np.mean(np.array(total_steps), axis=0)\n",
    "std_across_steps= np.std(np.array(total_steps), axis=0)\n",
    "\n",
    "avg_rewards=np.mean(np.array(total_rewards), axis=0)\n",
    "std_across_rewards= np.std(np.array(total_rewards), axis=0)\n",
    "\n",
    "avg_q=np.mean(np.array(Q_func), axis=0)\n",
    "avg_state=np.mean(np.array(state_visit), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming avg_steps, std_across_steps, avg_rewards, and std_across_rewards are defined\n",
    "\n",
    "episodes = np.arange(1, len(avg_steps) + 1)\n",
    "\n",
    "# Only include every 25th episode\n",
    "sampled_episodes = episodes[::10]\n",
    "sampled_avg_steps = avg_steps[::10]\n",
    "sampled_std_across_steps = std_across_steps[::10]\n",
    "sampled_avg_rewards = avg_rewards[::10]\n",
    "sampled_std_across_rewards = std_across_rewards[::10]\n",
    "\n",
    "# sampled_episodes = episodes\n",
    "# sampled_avg_steps = avg_steps\n",
    "# sampled_std_across_steps = std_across_steps\n",
    "# sampled_avg_rewards = avg_rewards\n",
    "# sampled_std_across_rewards = std_across_rewards\n",
    "\n",
    "# Plot for mean steps\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sampled_episodes, sampled_avg_steps, label='Mean Steps', color='blue')\n",
    "plt.fill_between(sampled_episodes, sampled_avg_steps - sampled_std_across_steps, \n",
    "                 sampled_avg_steps + sampled_std_across_steps, alpha=0.2, color='blue')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Steps')\n",
    "plt.title('Mean Steps and Standard Deviation Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot for mean rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sampled_episodes, sampled_avg_rewards, label='Mean Rewards', color='green')\n",
    "plt.fill_between(sampled_episodes, sampled_avg_rewards - sampled_std_across_rewards, \n",
    "                 sampled_avg_rewards + sampled_std_across_rewards, alpha=0.2, color='green')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Rewards')\n",
    "plt.title('Mean Rewards and Standard Deviation Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_gridworld(env_0, policy=pi, state_counts=avg_state, title=\"SARSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_gridworld(env_0, policy=pi,value_function=avg_q, title=\"SARSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "restart_states = np.array([[3,7],[8,2]])\n",
    "start_state = np.array([[0,4]])\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "# create model\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                    bad_states=bad_states,\n",
    "                    restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "               goal_reward=10,\n",
    "               bad_state_reward=-6,\n",
    "               restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=0.7,\n",
    "                              bias=0.5)\n",
    "\n",
    "env_1 = gw.create_gridworld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 0.5, log=True)\n",
    "    epsilon = trial.suggest_float('epsilon', 0.1, 0.5, log=True)\n",
    "    g=trial.suggest_float('gamma',0.8,1,log=True)\n",
    "\n",
    "    print(f\"Trial {trial.number}: Alpha = {alpha}, Epsilon = {epsilon},Gamma={g}\")\n",
    "\n",
    "    num_expts = 5\n",
    "    total_rewards, total_steps, Q_func, state_visit = [], [], [], []\n",
    "    gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False)\n",
    "    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                        bad_states=bad_states,\n",
    "                        restart_states=restart_states)\n",
    "    gw.add_rewards(step_reward=-1,\n",
    "                goal_reward=10,\n",
    "                bad_state_reward=-6,\n",
    "                restart_state_reward=-100)\n",
    "    gw.add_transition_probability(p_good_transition=0.7,\n",
    "                                bias=0.5)\n",
    "\n",
    "    env_1 = gw.create_gridworld()\n",
    "\n",
    "    for i in range(num_expts):\n",
    "        print(\"Experiment: %d\" % (i + 1))\n",
    "\n",
    "        q_function, pi, state_counts, steps, rewards = sarsa(env_1, alpha=alpha, epsilon=epsilon, maxiter=100, maxeps=5000)\n",
    "        total_steps.append(steps)\n",
    "        total_rewards.append(rewards)\n",
    "        Q_func.append(q_function)\n",
    "        state_visit.append(state_counts)\n",
    "\n",
    "    # Define a weighted combination of minimizing steps and maximizing rewards\n",
    "    # Adjust the weights based on your preference\n",
    "    objective_value = 0.5 * np.sum(np.sum(np.array(total_steps)),axis=0) / num_expts + 0.5 * (-np.sum(np.sum(np.array(total_rewards))) / num_expts)\n",
    "\n",
    "    return objective_value\n",
    "\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the combined objective\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the best hyperparameters\n",
    "\n",
    "\n",
    "best_alpha = study.best_params['alpha']\n",
    "best_epsilon = study.best_params['epsilon']\n",
    "best_gamma=study.best_params['gamma']\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(\"Best Alpha:\", best_alpha)\n",
    "print(\"Best Epsilon:\", best_epsilon)\n",
    "print(\"Best gamma:\", best_gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_expts = 5\n",
    "total_rewards, total_steps,Q_func,state_visit = [], [],[],[]\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False,gamma=best_gamma)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                        bad_states=bad_states,\n",
    "                        restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "                goal_reward=10,\n",
    "                bad_state_reward=-6,\n",
    "                restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=0.7,\n",
    "                                bias=0.5)\n",
    "\n",
    "env_1 = gw.create_gridworld()\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "\n",
    "    # TODO: run sarsa, store metrics\n",
    "    #Q, episode_rewards, steps_to_completion = qlearning(env, Q, choose_action=choose_action_softmax)\n",
    "    q_function, pi, state_counts,steps,rewards = sarsa(env_1, alpha=best_alpha, epsilon=best_epsilon, maxiter=100, maxeps=5000)\n",
    "    total_steps.append(steps)\n",
    "    total_rewards.append(rewards)\n",
    "    Q_func.append(q_function)\n",
    "    state_visit.append(state_counts)\n",
    "\n",
    "\n",
    "#steps_average=sum(steps)/len(steps)\n",
    "#reward_average=sum(rewards)/len(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_steps=np.mean(np.array(total_steps), axis=0)\n",
    "std_across_steps= np.std(np.array(total_steps), axis=0)\n",
    "\n",
    "avg_rewards=np.mean(np.array(total_rewards), axis=0)\n",
    "std_across_rewards= np.std(np.array(total_rewards), axis=0)\n",
    "\n",
    "avg_q=np.mean(np.array(Q_func), axis=0)\n",
    "avg_state=np.mean(np.array(state_visit), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming avg_steps, std_across_steps, avg_rewards, and std_across_rewards are defined\n",
    "\n",
    "episodes = np.arange(1, len(avg_steps) + 1)\n",
    "\n",
    "# Only include every 25th episode\n",
    "sampled_episodes = episodes[::10]\n",
    "sampled_avg_steps = avg_steps[::10]\n",
    "sampled_std_across_steps = std_across_steps[::10]\n",
    "sampled_avg_rewards = avg_rewards[::10]\n",
    "sampled_std_across_rewards = std_across_rewards[::10]\n",
    "\n",
    "# sampled_episodes = episodes\n",
    "# sampled_avg_steps = avg_steps\n",
    "# sampled_std_across_steps = std_across_steps\n",
    "# sampled_avg_rewards = avg_rewards\n",
    "# sampled_std_across_rewards = std_across_rewards\n",
    "\n",
    "# Plot for mean steps\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sampled_episodes, sampled_avg_steps, label='Mean Steps', color='blue')\n",
    "plt.fill_between(sampled_episodes, sampled_avg_steps - sampled_std_across_steps, \n",
    "                 sampled_avg_steps + sampled_std_across_steps, alpha=0.2, color='blue')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Steps')\n",
    "plt.title('Mean Steps and Standard Deviation Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot for mean rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sampled_episodes, sampled_avg_rewards, label='Mean Rewards', color='green')\n",
    "plt.fill_between(sampled_episodes, sampled_avg_rewards - sampled_std_across_rewards, \n",
    "                 sampled_avg_rewards + sampled_std_across_rewards, alpha=0.2, color='green')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Rewards')\n",
    "plt.title('Mean Rewards and Standard Deviation Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_gridworld(env_1, policy=pi, state_counts=avg_state, title=\"SARSA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_gridworld(env_1, policy=pi,value_function=avg_q, title=\"SARSA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 0.5, log=True)\n",
    "    temp = trial.suggest_float('temperature',0.1,1, log=True)\n",
    "    g=trial.suggest_float('gamma',0.8,1,log=True)\n",
    "\n",
    "    print(f\"Trial {trial.number}: Alpha = {alpha}, Temperature = {temp},Gamma={g}\")\n",
    "\n",
    "    num_expts = 5\n",
    "    total_rewards, total_steps, Q_func, state_visit = [], [], [], []\n",
    "    gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False,gamma=g)\n",
    "    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                            bad_states=bad_states,\n",
    "                            restart_states=restart_states)\n",
    "    gw.add_rewards(step_reward=-1,\n",
    "                    goal_reward=10,\n",
    "                    bad_state_reward=-6,\n",
    "                    restart_state_reward=-100)\n",
    "    gw.add_transition_probability(p_good_transition=0.7,\n",
    "                                    bias=0.5)\n",
    "\n",
    "    env_1 = gw.create_gridworld()\n",
    "\n",
    "    for i in range(num_expts):\n",
    "        print(\"Experiment: %d\" % (i + 1))\n",
    "\n",
    "        q_function, pi, state_counts,steps,rewards = sarsa(env_1, alpha=alpha, epsilon=0.2, maxiter=100, maxeps=5000,action_strategy=\"softmax\",temperature=temp)\n",
    "\n",
    "        total_steps.append(steps)\n",
    "        total_rewards.append(rewards)\n",
    "        Q_func.append(q_function)\n",
    "        state_visit.append(state_counts)\n",
    "\n",
    "    # Define a weighted combination of minimizing steps and maximizing rewards\n",
    "    # Adjust the weights based on your preference\n",
    "    objective_value = 0.5 * np.sum(np.sum(np.array(total_steps)),axis=0) / num_expts + 0.5 * (-np.sum(np.sum(np.array(total_rewards))) / num_expts)\n",
    "\n",
    "    return objective_value\n",
    "\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the combined objective\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the best hyperparameters\n",
    "\n",
    "\n",
    "best_alpha = study.best_params['alpha']\n",
    "best_temp = study.best_params['temperature']\n",
    "best_gamma=study.best_params['gamma']\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(\"Best Alpha:\", best_alpha)\n",
    "print(\"Best temp:\", best_temp)\n",
    "print(\"Best gamma:\", best_gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_expts = 5\n",
    "total_rewards, total_steps,Q_func,state_visit = [], [],[],[]\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False,gamma=best_gamma)\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                        bad_states=bad_states,\n",
    "                        restart_states=restart_states)\n",
    "gw.add_rewards(step_reward=-1,\n",
    "                goal_reward=10,\n",
    "                bad_state_reward=-6,\n",
    "                restart_state_reward=-100)\n",
    "gw.add_transition_probability(p_good_transition=0.7,\n",
    "                                bias=0.5)\n",
    "\n",
    "env_1 = gw.create_gridworld()\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "\n",
    "    # TODO: run sarsa, store metrics\n",
    "    #Q, episode_rewards, steps_to_completion = qlearning(env, Q, choose_action=choose_action_softmax)\n",
    "    q_function, pi, state_counts,steps,rewards = sarsa(env_1, alpha=best_alpha, epsilon=0.2, maxiter=100, maxeps=5000,action_strategy=\"softmax\",temperature=best_temp)\n",
    "    total_steps.append(steps)\n",
    "    total_rewards.append(rewards)\n",
    "    Q_func.append(q_function)\n",
    "    state_visit.append(state_counts)\n",
    "\n",
    "\n",
    "#steps_average=sum(steps)/len(steps)\n",
    "#reward_average=sum(rewards)/len(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_steps=np.mean(np.array(total_steps), axis=0)\n",
    "std_across_steps= np.std(np.array(total_steps), axis=0)\n",
    "\n",
    "avg_rewards=np.mean(np.array(total_rewards), axis=0)\n",
    "std_across_rewards= np.std(np.array(total_rewards), axis=0)\n",
    "\n",
    "avg_q=np.mean(np.array(Q_func), axis=0)\n",
    "avg_state=np.mean(np.array(state_visit), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming avg_steps, std_across_steps, avg_rewards, and std_across_rewards are defined\n",
    "\n",
    "episodes = np.arange(1, len(avg_steps) + 1)\n",
    "\n",
    "# Only include every 25th episode\n",
    "sampled_episodes = episodes[::10]\n",
    "sampled_avg_steps = avg_steps[::10]\n",
    "sampled_std_across_steps = std_across_steps[::10]\n",
    "sampled_avg_rewards = avg_rewards[::10]\n",
    "sampled_std_across_rewards = std_across_rewards[::10]\n",
    "\n",
    "# sampled_episodes = episodes\n",
    "# sampled_avg_steps = avg_steps\n",
    "# sampled_std_across_steps = std_across_steps\n",
    "# sampled_avg_rewards = avg_rewards\n",
    "# sampled_std_across_rewards = std_across_rewards\n",
    "\n",
    "# Plot for mean steps\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sampled_episodes, sampled_avg_steps, label='Mean Steps', color='blue')\n",
    "plt.fill_between(sampled_episodes, sampled_avg_steps - sampled_std_across_steps, \n",
    "                 sampled_avg_steps + sampled_std_across_steps, alpha=0.2, color='blue')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Steps')\n",
    "plt.title('Mean Steps and Standard Deviation Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot for mean rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sampled_episodes, sampled_avg_rewards, label='Mean Rewards', color='green')\n",
    "plt.fill_between(sampled_episodes, sampled_avg_rewards - sampled_std_across_rewards, \n",
    "                 sampled_avg_rewards + sampled_std_across_rewards, alpha=0.2, color='green')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Rewards')\n",
    "plt.title('Mean Rewards and Standard Deviation Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "plot_gridworld(env_1, policy=pi, state_counts=avg_state, title=\"SARSA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_gridworld(env_1, policy=pi,value_function=avg_q, title=\"SARSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
