{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veG1bzpubYmx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import gym\n",
        "from tqdm import tqdm_notebook\n",
        "import numpy as np\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtB5ZD1wbexX",
        "outputId": "0f4ff23e-5f66-4b99-b631-9081362bdb63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "DISCOUNT_FACTOR = 0.99\n",
        "NUM_EPISODES = 1000\n",
        "\n",
        "MAX_STEPS = 1000\n",
        "SOLVED_SCORE =-90\n",
        "\n",
        "#device to run model on\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqwABbkdbgsn"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size=6, h_size=16, a_size=3):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item() - 1, m.log_prob(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Use09hIrbjvr"
      },
      "outputs": [],
      "source": [
        "class StateValueNetwork(nn.Module):\n",
        "\n",
        "    #Takes in state\n",
        "    def __init__(self, observation_space):\n",
        "        super(StateValueNetwork, self).__init__()\n",
        "\n",
        "        self.input_layer = nn.Linear(observation_space, 128)\n",
        "        self.output_layer = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #input layer\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        #activiation relu\n",
        "        x = F.relu(x)\n",
        "\n",
        "        #get state value\n",
        "        state_value = self.output_layer(x)\n",
        "\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5DkFWkZbmAB"
      },
      "outputs": [],
      "source": [
        "def select_action(network, state):\n",
        "\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "    action_probs = network(state)\n",
        "    state = state.detach()\n",
        "\n",
        "    m = Categorical(action_probs)\n",
        "    action = m.sample()\n",
        "\n",
        "    #return action\n",
        "    return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2q0cwfbboZN",
        "outputId": "9670f93a-8165-44a7-fe88-1a735b1b043d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "env = gym.make('Acrobot-v1')\n",
        "\n",
        "#Init network\n",
        "policy_network = PolicyNetwork(env.observation_space.shape[0], env.action_space.n).to(DEVICE)\n",
        "stateval_network = StateValueNetwork(env.observation_space.shape[0]).to(DEVICE)\n",
        "\n",
        "#Init optimizer\n",
        "policy_optimizer = optim.SGD(policy_network.parameters(), lr=0.01)\n",
        "stateval_optimizer = optim.SGD(stateval_network.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uElEMbLjbtWU"
      },
      "outputs": [],
      "source": [
        "def reinforce():\n",
        "\n",
        "  scores = []\n",
        "\n",
        "  #track recent scores\n",
        "  recent_scores = deque(maxlen = 100)\n",
        "\n",
        "  #run episodes\n",
        "  for episode in tqdm_notebook(range(NUM_EPISODES)):\n",
        "\n",
        "      #init variables\n",
        "      state = env.reset()\n",
        "      done = False\n",
        "      score = 0\n",
        "      I = 1\n",
        "\n",
        "      #run episode, update online\n",
        "      for step in range(MAX_STEPS):\n",
        "\n",
        "          #get action and log probability\n",
        "          action, lp = select_action(policy_network, state)\n",
        "\n",
        "          #step with action\n",
        "          new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "          #update episode score\n",
        "          score += reward\n",
        "\n",
        "          #get state value of current state\n",
        "          state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "          state_val = stateval_network(state_tensor)\n",
        "\n",
        "          #get state value of next state\n",
        "          new_state_tensor = torch.from_numpy(new_state).float().unsqueeze(0).to(DEVICE)\n",
        "          new_state_val = stateval_network(new_state_tensor)\n",
        "\n",
        "          #if terminal state, next state val is 0\n",
        "          if done:\n",
        "              new_state_val = torch.tensor([0]).float().unsqueeze(0).to(DEVICE)\n",
        "\n",
        "          #calculate value function loss with MSE\n",
        "          val_loss = F.mse_loss(reward + DISCOUNT_FACTOR * new_state_val, state_val)\n",
        "          val_loss *= I\n",
        "\n",
        "          #calculate policy loss\n",
        "          advantage = reward + DISCOUNT_FACTOR * new_state_val.item() - state_val.item()\n",
        "          policy_loss = -lp * advantage\n",
        "          policy_loss *= I\n",
        "\n",
        "          #Backpropagate policy\n",
        "          policy_optimizer.zero_grad()\n",
        "          policy_loss.backward(retain_graph=True)\n",
        "          policy_optimizer.step()\n",
        "\n",
        "          #Backpropagate value\n",
        "          stateval_optimizer.zero_grad()\n",
        "          val_loss.backward()\n",
        "          stateval_optimizer.step()\n",
        "\n",
        "          if done:\n",
        "              break\n",
        "\n",
        "          #move into new state, discount I\n",
        "          state = new_state\n",
        "          I *= DISCOUNT_FACTOR\n",
        "\n",
        "      #append episode score\n",
        "      scores.append(score)\n",
        "      recent_scores.append(score)\n",
        "\n",
        "      return scores\n",
        "\n",
        "      early stopping if we meet solved score goal\n",
        "      if np.array(recent_scores).mean() >= SOLVED_SCORE:\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX166b1BKOrN"
      },
      "outputs": [],
      "source": [
        "all_scores = []\n",
        "for seed in seeds:\n",
        "    print(\"started training with seed: \", seed)\n",
        "    _, scores = reinforce()\n",
        "    print(\"completed training with seed: \", seed)\n",
        "    all_scores.append(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7FXuDh8KShy"
      },
      "outputs": [],
      "source": [
        "avg=np.mean(all_scores,axis=0)\n",
        "var=np.var(all_scores,axis=0)\n",
        "c=np.arange(1,len(all_score[0]),1)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(c,avg,label=\"Average_rewards\",color=\"blue\")\n",
        "plt.fill_between(c,avg-np.sqrt(var),avg+np.sqrt(var),alpha=0.2,color=\"blue\")\n",
        "plt.title(\"REINFORCE (Acrobot)\")\n",
        "plt.xlabel(\"Episode Count\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
